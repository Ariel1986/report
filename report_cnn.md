## 卷积神经网络(CNN)前向传播

### CNN的总体结构
+ 若干的卷积层＋ReLU激活层，若干的池化层(pooling),DNN全连接层以及最后的用Softmax激活函数的输出层。
+ 已完成：
    + 卷积层，ReLU激活层，池化层。
+ TODO：
    + DNN全连接层 激活函数。

#### 输入层－－－－> 卷积层
+ 输入层的前向传播是CNN前向传播的第一步。实现时也是直接在main函数将输入传到了卷积层。
前向传播的过程公式表示为：a2=σ(z2)=σ(a1∗W2+b2)
+ 其中，右边的数字代表层数；∗代表卷积；b代表的是偏倚；σ是激活函数，实现中用的是ReLU。
+ 已完成：
    + 二维黑白图片的运算。输入层是一个5维矩阵，矩阵的值等于图片的各个像素位置的值。此时和卷积层相连的卷积核也是W也是矩阵。
+ TODO：
    + 1）RGB的彩色图片：此时输入input就是3个矩阵，分别对应R,G,B的矩阵。此时和卷积层相连的卷积核W就是一个张量，对应的最后一维的维度为3，即每个卷积核都是由3个矩阵组成的。
    + 2）加入偏倚值b。
    + 3）填充padding：做卷积的时候，为了更好的识别边缘，可以在输入矩阵的周围加上若干圈的0再进行卷积。padding的值为加圈的个数。
    + 4）步幅stride：在卷积的过程中每次移动的像素距离大小，实现过程中默认该值为1.
    + 5) 求解的过程中用的是滑窗的方式，效率相对比较低，可以把卷积操作转化为矩阵乘法。这样就可以高效的利用优化之后的矩阵乘法，可以参考Caffe中im2col的实现。


#### 卷积层 －－－－－－－> 池化层
+ 池化层的目的就是对输入的矩阵进行缩小概括。比如输入的矩阵是N＊N维，池化大小是K＊K区域，则得到的输出矩阵就是（N/K) * (N/K)维的。
+ 该函数实现的过程需要定义的模型参数：
    + 1）池化区域的大小K
    + 2）池化的标准，一般为max或average

+ 已完成：池化区域大小为2，标准为max的池化函数
+ TODO：池化区域和标准可以配置的池化函数。


#### 池化层 －－－－－－－－> 全连接层 （TODO）
+ 池化层的输出到flatten层把所有的元素“拍平”，然后全到全连接层

#### 全连接层 －－－－－－－>  输出层 （TODO）
+ 全连接层到输出层就是正常的神经元和神经元之间的邻接相连，通过softmax函数计算后输出到output，得到不同类别的盖绿值，输出概率值最大的即为该图片的类别。

### 前向传播算法小结
+ 输入：
    + 图片样本；
    + 卷积层的数据：
        + CNN模型的层数L和所有隐藏层的类型；
        + 卷积核的大小K（实现中直接给了卷积核的值）；
        + 卷积子矩阵的维度F（实现中直接给了子矩阵的值）；
        + 填充大小padding（实现中默认为0）；
        + 步幅stride（实现中默认为1）；

    + 池化层的数据：
        + 池化区域的大小
        + 池化标准（max或average）
    + 全连接层的数据：
        + 连接层的激活函数（输出层不需要）
        + 各层的神经元个数

+ 输出
    + CNN模型的输出a[L]

+ 处理过程
    + 1) 根据输入层的填充大小padding填充原始图片的边缘，得到输入a[1]
    + 2）初始化所有隐藏层的参数W，b
    + 3）for l＝2 to L－1
        + a) 如果第l层是卷积层，输出：a[l]=ReLU(z[l])=ReLU(a[l−1]∗W[l]+b[l])
        + b) 如果第l层是池化层，输出：a[l]=pool(a[l]−1) ,
        + c) 如果第l层是全连层，输出：a[l]=σ(z[l])=σ(W[l]a[l−1]+b[l])
    + 4）对于输出层第L层：a[L]=softmax(z[L])=softmax(W[L]a[L−1]+b[L])
+ 已完成：
    + 从输入到卷积层，卷积层到池化层的一次处理过程
+ TODO:
    + 全连层的实现。
    + 循环多次调用层。
    + 卷积过程中使用的padding和stride都是使用默认值，需要加入对两者的处理。
    + polling层中现在使用的是max_2*2,可以改成通用的实现方式。
    + 训练实现的模型。
